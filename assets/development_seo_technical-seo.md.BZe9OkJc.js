import{_ as a}from"./chunks/PageSpeed_HaunBlog_en.B8qqF5YQ.js";import{_ as t,c as o,a2 as i,o as s}from"./chunks/framework.wtg6z0M2.js";const b=JSON.parse('{"title":"SEO (Serach Engine Optimization); Technical SEO","description":"Technical SEO refers to the process of making a website easily recognizable by search engines. This includes elements like sitemaps, loading speed, and mobile compatibility.","frontmatter":{"title":"SEO (Serach Engine Optimization); Technical SEO","description":"Technical SEO refers to the process of making a website easily recognizable by search engines. This includes elements like sitemaps, loading speed, and mobile compatibility.","head":[["meta",{"property":"og:title","content":"SEO (Serach Engine Optimization); Technical SEO"}],["meta",{"property":"og:description","content":"Technical SEO refers to the process of making a website easily recognizable by search engines. This includes elements like sitemaps, loading speed, and mobile compatibility."}],["meta",{"property":"og:image","content":"/images/seo.png"}],["meta",{"property":"og:url","content":"https://haun84.github.io/development/seo/technical-seo"}]]},"headers":[],"relativePath":"development/seo/technical-seo.md","filePath":"en/development/seo/technical-seo.md","lastUpdated":null}'),l={name:"development/seo/technical-seo.md"};function r(n,e,c,d,p,h){return s(),o("div",null,e[0]||(e[0]=[i('<h1 id="technical-seo-guide" tabindex="-1">Technical SEO Guide <a class="header-anchor" href="#technical-seo-guide" aria-label="Permalink to &quot;Technical SEO Guide&quot;">​</a></h1><div class="info custom-block"><p class="custom-block-title">Language</p><ul><li><a href="/ko/development/seo/technical-seo">한국어 페이지: Korean Page</a></li></ul></div><h2 id="technical-seo-overview" tabindex="-1">Technical SEO Overview <a class="header-anchor" href="#technical-seo-overview" aria-label="Permalink to &quot;Technical SEO Overview&quot;">​</a></h2><ul><li><p>Technical SEO refers to the process of making a website easily recognizable by search engines. This includes elements like sitemaps, loading speed, and mobile compatibility.</p></li><li><p>Optimization is essential because the technical structure of a website has a profound impact on website performance. No matter how much high-quality content a website has, it cannot effectively deliver the content to users as well as search engines, unless it has a healthy and solid technical structure.</p></li><li><p>Below are the most basic <code>technical SEO</code> tasks to be applied.</p></li></ul><h2 id="apply-secure-protocol-https" tabindex="-1">Apply Secure Protocol (<code>HTTPS</code>) <a class="header-anchor" href="#apply-secure-protocol-https" aria-label="Permalink to &quot;Apply Secure Protocol (`HTTPS`)&quot;">​</a></h2><ul><li><p><code>Google</code> designates the secure protocol as a ranking factor so that all websites accessed by users through Google are secure, giving the website (<code>HTTPS</code>) with the secure protocol a higher score than the website (<code>HTTP</code>) without the secure protocol.</p></li><li><p>It is strongly recommended to apply the secure protocol (<code>HTTPS</code>) to all websites, but it is essential to apply it, especially for websites that collect financial transactions and user personal information on websites.</p></li><li><p>There are many ways to apply security protocols, but if a website is created using <code>GitHub</code> like this website, <code>HTTPS</code> can be easily applied for free, and security protocols can be applied using services such as <a href="https://www.cloudflare.com" target="_blank" rel="noreferrer">Cloud Flare</a>(CloudFlare).</p></li></ul><div class="warning custom-block"><p class="custom-block-title">HaunBlog by GitHub Pages</p><ul><li><code>HaunBlog</code> is applying a secure protocol (<code>HTTPS</code>) using <a href="https://docs.github.com/en/pages/quickstart" target="_blank" rel="noreferrer">GitHub Pages</a>, GitHub&#39;s website hosting service.</li></ul></div><h2 id="getting-mobile-friendly" tabindex="-1">Getting Mobile Friendly <a class="header-anchor" href="#getting-mobile-friendly" aria-label="Permalink to &quot;Getting Mobile Friendly&quot;">​</a></h2><ul><li><p><code>Mobile-centric indexing</code> means indexing mobile version of content first. As mobile search volume surpassed desktop search volume, Google began creating mobile-centric indexes in earnest. As a result, the mobile affinity of websites has become a very important ranking factor.</p></li><li><p>The mobile affinity of the website can be found through <a href="https://developer.chrome.com/docs/lighthouse/overview" target="_blank" rel="noreferrer">Google&#39;s Mobile Affinity Test Tool</a> , and if the website does not support the mobile version, the website should be improved to mobile affinity using <a href="https://support.google.com/webdesigner/answer/7002913?hl=en" target="_blank" rel="noreferrer">Response Web Design</a>.</p></li><li><p>If you are using a mobile-only website (e.g., m.example.com ), you must apply a <a href="https://developers.google.com/search/docs/crawling-indexing/consolidate-duplicate-urls" target="_blank" rel="noreferrer">canonical tag</a> that informs you of the correlation between the mobile website and the desktop website.</p></li></ul><h2 id="fast-page-loading" tabindex="-1">Fast Page Loading <a class="header-anchor" href="#fast-page-loading" aria-label="Permalink to &quot;Fast Page Loading&quot;">​</a></h2><ul><li><p>Like mobile affinity, page loading speed has a huge impact on the user experience, making it one of Google&#39;s most important ranking factors along with security protocols and mobile affinity.</p></li><li><p>Google believes that websites with loading speeds of less than 3 seconds for desktops and less than 2 seconds for mobile are the most competitive. The website loading speed can be diagnosed through the <a href="https:///pagespeed.web.dev" target="_blank" rel="noreferrer">PageSpeed Insights</a> tool provided by Google.</p></li></ul><div class="warning custom-block"><p class="custom-block-title">HaunBlog by VitePress</p><ul><li><code>HaunBlog</code> was created through a <a href="https://en.wikipedia.org/wiki/Static_site_generator" target="_blank" rel="noreferrer">static site generator (SSG)</a> designed to build a fast and content-oriented website called <a href="https://vitepress.dev/" target="_blank" rel="noreferrer">VitePress</a> and is applying high mobile affinity, fast page loading speed, and website structure search engine optimization.</li></ul><p><img src="'+a+'" alt="PageSpeed_HaunBlog"></p></div><h2 id="to-generate-a-url-structurally" tabindex="-1">To generate a URL structurally <a class="header-anchor" href="#to-generate-a-url-structurally" aria-label="Permalink to &quot;To generate a URL structurally&quot;">​</a></h2><ul><li>When the search engine understands the structure of the site, it also reviews the elements of the URL. If all posts are connected only to the main domain, it will be configured as follows.</li></ul><blockquote><p><a href="https://www.example.com/SEO-Guide" target="_blank" rel="noreferrer">https://www.example.com/SEO-Guide</a><br><a href="https://www.example.com/Service-Introduction" target="_blank" rel="noreferrer">https://www.example.com/Service-Introduction</a><br><a href="https://www.example.com/SEO-Marketing-Guide" target="_blank" rel="noreferrer">https://www.example.com/SEO-Marketing-Guide</a></p></blockquote><ul><li>In the above case, the content is different, but all are grouped only by <a href="https://www.example.com" target="_blank" rel="noreferrer">https://www.example.com</a> . However, if you use URLs structurally, you can group categories to create a more SEO-friendly structure.</li></ul><blockquote><p><a href="https://www.example.com/guide/SEO-Guide" target="_blank" rel="noreferrer">https://www.example.com/guide/SEO-Guide</a><br><a href="https://www.example.com/service/Service-Introduction" target="_blank" rel="noreferrer">https://www.example.com/service/Service-Introduction</a><br><a href="https://www.example.com/marketing/SEO-Marketing-Guide" target="_blank" rel="noreferrer">https://www.example.com/marketing/SEO-Marketing-Guide</a></p></blockquote><ul><li><p>In the above case, it is divided into <code>guide</code>, <code>service</code>, and <code>marketing</code> folders, and it helps the crawling bot understand the structure of the website.</p></li><li><p>It is also SEO-friendly to set the URL as a word related to the subject of the content.</p></li></ul><h2 id="create-a-sitemap-sitemap-xml-and-update-it-periodically" tabindex="-1">Create a Sitemap (<code>sitemap.xml</code>) and update it periodically <a class="header-anchor" href="#create-a-sitemap-sitemap-xml-and-update-it-periodically" aria-label="Permalink to &quot;Create a Sitemap (`sitemap.xml`) and update it periodically&quot;">​</a></h2><ul><li><p>You can think of a <code>sitemap</code> as a map that organizes all web pages issued by a website into a long list.</p></li><li><p><code>sitemap</code> help search engines index all web pages in the process of visiting websites and crawling.</p></li><li><p>Therefore, it is necessary to periodically update the <code>sitemap</code> by adding the URL to the <code>sitemap</code> whenever new content is published on the website.</p></li><li><p>For Google SEO, the <code>sitemap</code> can be updated directly on the <a href="https://search.google.com/search-console" target="_blank" rel="noreferrer">Google Search Console</a>.</p></li><li><p>For more information on how to create and submit a <code>sitemap</code>, visit <a href="https://developers.google.com/search/docs/crawling-indexing/sitemaps/build-sitemap" target="_blank" rel="noreferrer">Google Guidelines</a> .</p></li></ul><div class="warning custom-block"><p class="custom-block-title">HaunBlog sitemap</p><ul><li>The <code>sitemap</code> of <code>HaunBlog</code> is available at <a href="https://haun84.github.io/sitemap.xml" target="_blank" rel="noreferrer">https://haun84.github.io/sitemap.xml</a></li></ul></div><h2 id="create-robots-exclusion-standard-file-robots-txt" tabindex="-1">Create <code>Robots Exclusion Standard File</code> (<code>robots.txt</code>) <a class="header-anchor" href="#create-robots-exclusion-standard-file-robots-txt" aria-label="Permalink to &quot;Create `Robots Exclusion Standard File` (`robots.txt`)&quot;">​</a></h2><ul><li><p><a href="https://developers.google.com/search/docs/crawling-indexing/robots/create-robots-txt" target="_blank" rel="noreferrer">Robots Exclusion Standard File</a> means a file that blocks or allows access to a crawler (robot) who crawls a search engine.</p></li><li><p>At this time, not only Google, but all search engines such as Naver, Yandex, Bing, Daum, and Yahoo are applicable, and all search engine robots can issue common commands, and different commands can be issued for each search engine robot.</p></li></ul><div class="tip custom-block"><p class="custom-block-title">Allow all</p><ul><li>For example, if you want to allow all pages of the website to be indexed to all search engine crawlers, you can create a standard file for robot exclusion as shown below.</li></ul><div class="language-txt vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">txt</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>User-agent: *</span></span>\n<span class="line"><span>Allow: /</span></span></code></pre></div></div><div class="warning custom-block"><p class="custom-block-title">Disallow Partly</p><ul><li>Conversely, to issue an unindexed command from all search engine crawlers (robot) to avoid exposing the website&#39;s administrator page (e.g., www.example.com/admin/) to the search results page, you can create a robot exclusion standard file as follows.</li></ul><div class="language-txt vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">txt</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>User-agent: *</span></span>\n<span class="line"><span>Disallow: /admin/</span></span></code></pre></div></div><div class="danger custom-block"><p class="custom-block-title">Disallow All</p><ul><li>It should be noted that all crawlers should be prohibited from indexing all content if written as below.</li></ul><div class="language-txt vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">txt</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>User-agent: *</span></span>\n<span class="line"><span>Disallow: /</span></span></code></pre></div></div>',26)]))}const m=t(l,[["render",r]]);export{b as __pageData,m as default};
